import streamlit as st
import vertexai
from vertexai.generative_models import GenerativeModel
from google.oauth2 import service_account

# --- 1. èªè¨¼ã¨åˆæœŸåŒ– ---
if "gcp_service_account" in st.secrets:
    info = st.secrets["gcp_service_account"]
    credentials = service_account.Credentials.from_service_account_info(info)
    vertexai.init(project=info["project_id"], location="us-central1", credentials=credentials)
else:
    st.error("Secretsè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

# --- 2. ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ ---
model_path = "projects/180827076471/locations/us-central1/endpoints/4782082832941973504"
model = GenerativeModel(model_path)

# --- 3. ç”»é¢ãƒ‡ã‚¶ã‚¤ãƒ³ ---
# ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã€Œåå‰ã§ãƒã‚¨ãƒ ï¼ã€ã«å¤‰æ›´
st.title("ğŸŒ¸ åå‰ã§ãƒã‚¨ãƒ ï¼")
st.write("ãƒ—ãƒ­ã®ä½œé¢¨ã‚’å­¦ç¿’ã—ãŸAIãŒã€ãŠåå‰ã«åˆã‚ã›ãŸ5ã€œ6è¡Œã®è©©ã‚’ä½œæˆã—ã¾ã™ã€‚")

name = st.text_input("ãŠåå‰ï¼ˆæ¼¢å­—ï¼‰", "å°äº”éƒ")
profile = st.text_area("äººç‰©ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ï¼ˆæ€§æ ¼ã‚„è¶£å‘³ãªã©ï¼‰", "å‹‡ã¾ã—ã„å¤§å·¥ã•ã‚“ã€‚ãƒ”ã‚¢ãƒã‚‚å¾—æ„ã€‚")

# ç”¨é€”ã®é¸æŠè‚¢ã‚’ç´°åˆ†åŒ–
usage_list = ["èª•ç”Ÿæ—¥", "é‚„æš¦ç¥", "å¤å¸Œç¥", "é•·å¯¿ç¥", "é€€è·ç¥ã„", "çµå©šç¥ã„", "æˆäººç¥", "ãã®ä»–"]
usage_choice = st.selectbox("ç”¨é€”", usage_list)

final_usage = usage_choice
if usage_choice == "ãã®ä»–":
    custom_usage = st.text_input("ãŠç¥ã„ã®ç›®çš„ã‚’è‡ªç”±ã«å…¥åŠ›ã—ã¦ãã ã•ã„")
    final_usage = custom_usage

# --- 4. ç”Ÿæˆå®Ÿè¡Œ ---
if st.button("è©©ã‚’ä½œæˆã™ã‚‹"):
    with st.spinner("ãƒ—ãƒ­ã®ä½œé¢¨ã‚’å†ç¾ä¸­..."):
        # ã€é‡è¦ã€‘å­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å®Œå…¨ã«å†ç¾ã—ã€ã€Œè©©ã ã‘ã‚’å‡ºåŠ›ã›ã‚ˆã€ã¨å¼·ãå‘½ã˜ã¾ã™
        # æ¼¢å­—ã®è¾ºã‚„ä½œã‚Šã‚’æ´»ç”¨ã™ã‚‹æŒ‡ç¤ºã‚‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ„ã¿è¾¼ã¿ã¾ã—ãŸ
        prompt = (
            f"ä»¥ä¸‹ã‚’å…ƒã«ã€å­¦ç¿’ã—ãŸä½œé¢¨ã§5ã€œ6è¡Œã®ãƒãƒ¼ãƒ ã‚¤ãƒ³ãƒã‚¨ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
            f"ä½™è¨ˆãªæŒ¨æ‹¶ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚è©©ã®å†…å®¹ã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
            f"æ¼¢å­—ï¼š{name}\n"
            f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ï¼š{profile}\n"
            f"ç”¨é€”ï¼š{final_usage}"
        )
        
        # å‰µé€ æ€§ã‚’å°‘ã—æŠ‘ãˆã¦ï¼ˆtemperature=0.4ï¼‰ã€å­¦ç¿’ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¿ å®Ÿã«å®ˆã‚‰ã›ã¾ã™
        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 512, "temperature": 0.4}
        )
        
        st.subheader("ç”Ÿæˆã•ã‚ŒãŸãƒã‚¨ãƒ ")
        # è©©ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
        st.success(response.text)
